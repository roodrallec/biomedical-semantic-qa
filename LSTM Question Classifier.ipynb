{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# DEPENDENCIES \n",
    "###\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from matplotlib import pyplot as pltb\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib notebook  \n",
    "### \n",
    "# GLOBALS \n",
    "###\n",
    "TRAINING_DATA_DIR=\"./datasets/BioASQ-trainingDataset6b.json\"\n",
    "TRAIN_SPLIT = 0.9\n",
    "W2V_SIZE=50\n",
    "W2V_SKIP_GRAM=1\n",
    "### \n",
    "# FUNCTIONS \n",
    "###\n",
    "def parse_questions_types(data):\n",
    "    return zip(*[[json['body'], json['type']] for json in data['questions']])\n",
    "\n",
    "def label_to_class(str_labels, label):\n",
    "    return str_labels.index(label)\n",
    "\n",
    "def json_to_df(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return pd.DataFrame(json.load(f))\n",
    "    \n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    filtered_tokens = []\n",
    "    for word, pos in nltk.pos_tag(word_tokenize(text)):\n",
    "#         if len(word) < 2 and word != \"?\":\n",
    "#             continue\n",
    "        filtered_tokens.append(word.lower())        \n",
    "    return filtered_tokens\n",
    "\n",
    "def build_vocab_idx(tokenized_q):\n",
    "    vocab = dict()\n",
    "    idx = 0\n",
    "    for q in tokenized_q:\n",
    "        for word in q:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "    return vocab\n",
    "\n",
    "def one_hot_encode(q_types):\n",
    "    y = []\n",
    "    list_classes = list(np.unique(q_types))\n",
    "    for q_type in q_types:\n",
    "        enc = np.zeros(len(list_classes))\n",
    "        enc[list_classes.index(q_type)] = 1\n",
    "        y.append(enc)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer spatial_dropout1d_11 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[ 3.37759525e-01,  1.36463165e-01,  9.89217460e-02, ...,\n        -4.89831604e-02, -1.76981464e-01,  1.16860926e-01],\n       [ 1.36913821e-01,  6.65977672e-02,  6.44709319e-02, ...,\n        -4.51426916e-02, -8.68188888e-02, -1.86762027e-02],\n       [ 3.18445861e-01,  1.35832369e-01,  1.40888259e-01, ...,\n        -7.52645209e-02, -1.98622227e-01,  1.64264441e-02],\n       ...,\n       [ 3.92016806e-02,  1.47319147e-02,  9.53727867e-03, ...,\n        -6.55975984e-03, -2.37124078e-02,  2.07531339e-04],\n       [ 4.59352396e-02,  1.40201114e-02,  2.28720028e-02, ...,\n        -4.26497497e-03, -2.94809937e-02, -5.46028232e-03],\n       [ 2.08940078e-02,  8.19153339e-03,  4.78963973e-03, ...,\n        -7.11355032e-03, -1.76139902e-02, -7.87751470e-03]])]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 474\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9fb3a26cfdeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# biGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer spatial_dropout1d_11 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[ 3.37759525e-01,  1.36463165e-01,  9.89217460e-02, ...,\n        -4.89831604e-02, -1.76981464e-01,  1.16860926e-01],\n       [ 1.36913821e-01,  6.65977672e-02,  6.44709319e-02, ...,\n        -4.51426916e-02, -8.68188888e-02, -1.86762027e-02],\n       [ 3.18445861e-01,  1.35832369e-01,  1.40888259e-01, ...,\n        -7.52645209e-02, -1.98622227e-01,  1.64264441e-02],\n       ...,\n       [ 3.92016806e-02,  1.47319147e-02,  9.53727867e-03, ...,\n        -6.55975984e-03, -2.37124078e-02,  2.07531339e-04],\n       [ 4.59352396e-02,  1.40201114e-02,  2.28720028e-02, ...,\n        -4.26497497e-03, -2.94809937e-02, -5.46028232e-03],\n       [ 2.08940078e-02,  8.19153339e-03,  4.78963973e-03, ...,\n        -7.11355032e-03, -1.76139902e-02, -7.87751470e-03]])]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "tokenized_q = [tokenize(q) for q in questions]\n",
    "vocab_idx = build_vocab_idx(tokenized_q)\n",
    "vector_model = Word2Vec(tokenized_q, min_count=0, sg=W2V_SKIP_GRAM, size=W2V_SIZE)\n",
    "max_nb_words = len(vector_model.wv.vocab)\n",
    "max_seq_length = max([len(q) for q in tokenized_q])\n",
    "sequences = [[vocab_idx[t] for t in q] for q in tokenized_q]\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\")\n",
    "y = one_hot_encode(q_types)\n",
    "train_q_idx = round(len(X)*TRAIN_SPLIT)\n",
    "X_train, y_train = X[:train_q_idx], y[:train_q_idx]\n",
    "X_test, y_test = X[train_q_idx:], y[train_q_idx:]\n",
    "\n",
    "\n",
    "wv_matrix = (np.random.rand(max_nb_words, W2V_SIZE) - 0.5) / 5.0\n",
    "\n",
    "for word, i in vocab_idx.items():\n",
    "    wv_matrix[i] = vector_model.wv[word]\n",
    "\n",
    "wv_layer = Embedding(max_nb_words,\n",
    "                     W2V_SIZE,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=max_seq_length,\n",
    "                     trainable=False)\n",
    "\n",
    "# Inputs\n",
    "sequence_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "embedded_sequences = wv_layer(sequence_input)\n",
    "\n",
    "# biGRU\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output\n",
    "# x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(4, activation='sigmoid')(x)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[sequence_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=[])\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_split=0.2, epochs=30, batch_size=256, shuffle=True)\n",
    "\n",
    "history = pd.DataFrame(hist.history)\n",
    "plt.figure(figsize=(5,5));\n",
    "plt.plot(history[\"loss\"], 'r');\n",
    "plt.plot(history[\"val_loss\"], 'b');\n",
    "plt.title(\"Loss with pretrained word vectors\");\n",
    "plt.show();\n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_hat, axis=1), target_names=np.unique(q_types)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
