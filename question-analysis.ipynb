{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# DEPENDENCIES \n",
    "###\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import Tree\n",
    "# gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "# plotting\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.widgets import CheckButtons\n",
    "from wordcloud import WordCloud\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook  \n",
    "# stanford corenlp\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "### \n",
    "# GLOBALS \n",
    "###\n",
    "TRAINING_DATA_DIR=\"./datasets/BioASQ-trainingDataset6b.json\"\n",
    "# TFIDF CONFIG\n",
    "TOP_N=5\n",
    "MIN_DF=0.01\n",
    "MAX_DF=1.00\n",
    "N_GRAMS=[(1,1),(2,2),(3,3),(1,3)]\n",
    "BUILD_WORD_CLOUD=False\n",
    "# Stanford core-nlp config\n",
    "CORE_NLP_CLIENT = StanfordCoreNLP(r'/Users/jalexander/Projects/stanford-corenlp-full-2018-10-05')\n",
    "### \n",
    "# FUNCTIONS \n",
    "###\n",
    "def build_vector_model(questions):\n",
    "    questions = [tokenize(q) for q in questions]\n",
    "    model = Word2Vec(questions, min_count=0, sg=0)\n",
    "    question_vectors = []\n",
    "    for question in questions:\n",
    "        question_vector = []\n",
    "        for word in question:\n",
    "            question_vector = model[word] if (len(question_vector) == 0) else np.add(question_vector, model[word])\n",
    "        question_vectors.append(question_vector)\n",
    "    return question_vectors        \n",
    "\n",
    "def parse_questions_types(data):\n",
    "    return zip(*[[json['body'], json['type']] for json in data['questions']])\n",
    "\n",
    "def label_to_class(str_labels, label):\n",
    "    return str_labels.index(label)\n",
    "\n",
    "def build_tfidf_weights(sent_list, min_df, max_df, ngram, top_n=25):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df, \n",
    "        min_df=min_df, \n",
    "        tokenizer=tokenize, \n",
    "        ngram_range=ngram\n",
    "    )\n",
    "    tfidf = tfidf_vectorizer.fit_transform(sent_list)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    return top_mean_feats(tfidf, terms, top_n=top_n)\n",
    "\n",
    "def build_word_cloud(tfidf_weights, output_file):\n",
    "    # Initialize the word cloud\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=1000,\n",
    "        width = 1024,\n",
    "        height = 720,\n",
    "    )\n",
    "    wc.generate_from_frequencies(tfidf_weights)\n",
    "    wc.to_file(output_file)\n",
    "\n",
    "def json_to_df(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return pd.DataFrame(json.load(f))\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    feats = {}\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    for i in topn_ids:\n",
    "        feats[features[i]] = row[i]\n",
    "\n",
    "    return feats\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    filtered_tokens = []\n",
    "    for word, pos in nltk.pos_tag(word_tokenize(text)):\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        filtered_tokens.append(word.lower())        \n",
    "    return filtered_tokens\n",
    "\n",
    "def build_parse_tree(text):\n",
    "    text = text.rstrip().split('. ')[-1]\n",
    "    return CORE_NLP_CLIENT.parse(text).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "\n",
    "for q in questions:\n",
    "    tree = build_parse_tree(q) \n",
    "    feature_set = []        \n",
    "    feature_set.append(1 if 'List' in tree else 0)\n",
    "    feature_set.append(1 if 'Is' in tree else 0)\n",
    "    feature_set.append(1 if 'WH' in tree else 0)\n",
    "    feature_set.append(1 if '?' in tree else 0)            \n",
    "    features.append(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.44      0.93      0.60        67\n",
      "        list       1.00      0.26      0.41        47\n",
      "     summary       0.67      0.12      0.20        51\n",
      "       yesno       0.92      0.98      0.95        61\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       226\n",
      "   macro avg       0.76      0.57      0.54       226\n",
      "weighted avg       0.74      0.62      0.56       226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.1)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TFIDF DATA ANALYSIS\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "print(\"Labels\")\n",
    "print(str_labels)\n",
    "print()\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "for label in str_labels:\n",
    "    label_questions = [questions[idx] for idx, q_type in enumerate(q_types) if q_type == label]\n",
    "    count = len(label_questions)   \n",
    "    average_question_length = np.average([len(q) for q in label_questions])\n",
    "    tfidf_weights = []\n",
    "\n",
    "    for ngram in N_GRAMS:\n",
    "        weights = build_tfidf_weights(label_questions, MIN_DF, MAX_DF, ngram, TOP_N) \n",
    "        tfidf_weights.append(weights)\n",
    "    \n",
    "        if BUILD_WORD_CLOUD:\n",
    "            build_word_cloud(weights, f\"{label}_{ngram}_world_cloud.png\")\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Label Analysis: {label}\")        \n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Average question length: {average_question_length}\")\n",
    "    for idx, ngram in enumerate(N_GRAMS):\n",
    "        print(f\"TFIDF NGRAM={ngram}: {tfidf_weights[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL BUILDING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "question_vectors = build_vector_model(questions)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(question_vectors)\n",
    "pca_res = pca.transform(question_vectors)\n",
    "\n",
    "# Plotting result\n",
    "pyplot.cla(); pyplot.clf();\n",
    "fig = pyplot.figure(1, figsize=(10, 10))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "colors = {\n",
    "    'factoid': 'red', \n",
    "    'list': 'blue', \n",
    "    'summary': 'green', \n",
    "    'yesno': 'black'\n",
    "}\n",
    "indexes = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "series = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "\n",
    "def check_func(label):\n",
    "    l_series = series[label]\n",
    "    l_series.set_visible(not l_series.get_visible())\n",
    "\n",
    "[indexes[label].append(idx) for idx, label in enumerate(q_types)]\n",
    "\n",
    "for label in str_labels:\n",
    "    idx = indexes[label]\n",
    "    series[label] = ax.scatter(pca_res[idx, 0], pca_res[idx, 1], pca_res[idx, 2], color=colors[label], s=8, alpha=0.4)\n",
    "\n",
    "rax = pyplot.axes([0.05, 0.4, 0.15, 0.15])\n",
    "check = CheckButtons(rax, ('factoid', 'list', 'summary', 'yesno'), (True, True, True, True))        \n",
    "[rec.set_facecolor(colors[label]) for label, rec in zip(str_labels, check.rectangles)]\n",
    "check.on_clicked(check_func)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL TESTING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "X = build_vector_model(questions)\n",
    "str_labels = list(np.unique(q_types))\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "# PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train)\n",
    "X_train_tr = pca.transform(X_train)\n",
    "X_test_tr = pca.transform(X_test)\n",
    "\n",
    "# search for optimal SVM parameters using grid search with 3-fold cross validation\n",
    "Cs = np.logspace(0, 4, 4)\n",
    "gammas = np.logspace(0, 3, 4)\n",
    "param_grid = {'C': Cs, 'kernel': ['linear','rbf'], 'gamma': gammas}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=param_grid)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(clf.best_estimator_)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN()\n",
    "clf = KNeighborsClassifier(n_neighbors=4)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB()\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
