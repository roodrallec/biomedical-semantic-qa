{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# DEPENDENCIES \n",
    "###\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import Tree\n",
    "# gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "# plotting\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.widgets import CheckButtons\n",
    "from wordcloud import WordCloud\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook  \n",
    "# stanford corenlp\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "### \n",
    "# GLOBALS \n",
    "###\n",
    "TRAINING_DATA_DIR=\"./datasets/BioASQ-trainingDataset6b.json\"\n",
    "# TFIDF CONFIG\n",
    "TOP_N=5\n",
    "MIN_DF=0.01\n",
    "MAX_DF=1.00\n",
    "N_GRAMS=[(1,1),(2,2),(3,3),(1,3)]\n",
    "BUILD_WORD_CLOUD=False\n",
    "# Stanford core-nlp config\n",
    "CORE_NLP_CLIENT = StanfordCoreNLP(r'/Users/jalexander/Projects/stanford-corenlp-full-2018-10-05')\n",
    "### \n",
    "# FUNCTIONS \n",
    "###\n",
    "def build_vector_model(questions):\n",
    "    questions = [tokenize(q) for q in questions]\n",
    "    model = Word2Vec(questions, min_count=0, sg=0)\n",
    "    question_vectors = []\n",
    "    for question in questions:\n",
    "        question_vector = []\n",
    "        for word in question:\n",
    "            question_vector = model[word] if (len(question_vector) == 0) else np.add(question_vector, model[word])\n",
    "        question_vectors.append(question_vector)\n",
    "    return question_vectors        \n",
    "\n",
    "def parse_questions_types(data):\n",
    "    return zip(*[[json['body'], json['type']] for json in data['questions']])\n",
    "\n",
    "def label_to_class(str_labels, label):\n",
    "    return str_labels.index(label)\n",
    "\n",
    "def build_tfidf_weights(sent_list, min_df, max_df, ngram, top_n=25):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df, \n",
    "        min_df=min_df, \n",
    "        tokenizer=tokenize, \n",
    "        ngram_range=ngram\n",
    "    )\n",
    "    tfidf = tfidf_vectorizer.fit_transform(sent_list)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    return top_mean_feats(tfidf, terms, top_n=top_n)\n",
    "\n",
    "def build_word_cloud(tfidf_weights, output_file):\n",
    "    # Initialize the word cloud\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=1000,\n",
    "        width = 1024,\n",
    "        height = 720,\n",
    "    )\n",
    "    wc.generate_from_frequencies(tfidf_weights)\n",
    "    wc.to_file(output_file)\n",
    "\n",
    "def json_to_df(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return pd.DataFrame(json.load(f))\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    feats = {}\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    for i in topn_ids:\n",
    "        feats[features[i]] = row[i]\n",
    "\n",
    "    return feats\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    filtered_tokens = []\n",
    "    for word, pos in nltk.pos_tag(word_tokenize(text)):\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        filtered_tokens.append(word.lower())        \n",
    "    return filtered_tokens\n",
    "\n",
    "def build_parse_tree(text):\n",
    "    text = text.rstrip().split('. ')[-1]\n",
    "    return CORE_NLP_CLIENT.parse(text).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (SQ (VBZ Is) (NP (NNP Hirschsprung) (NN disease)) (NP (NP (DT a) (JJ mendelian)) (CC or) (NP (DT a) (JJ multifactorial) (NN disorder))) (. ?)))\n",
      "(ROOT (NP (NP (NP (NN List) (NN signaling) (NNS molecules)) (PRN (-LRB- -LRB-) (NP (NNS ligands)) (-RRB- -RRB-)) (SBAR (WHNP (WDT that)) (S (VP (VBP interact) (PP (IN with) (NP (DT the) (NN receptor) (NN EGFR))))))) (. ?)))\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "c=0\n",
    "\n",
    "for q in questions:\n",
    "    tree = build_parse_tree(q) \n",
    "    c+=1\n",
    "    if c==3:\n",
    "        break\n",
    "    print(re.sub(r' +', ' ', tree))\n",
    "    feature_set = []        \n",
    "    feature_set.append(1 if 'List' in tree else 0)\n",
    "    feature_set.append(1 if 'Is' in tree else 0)\n",
    "    feature_set.append(1 if 'WH' in tree else 0)\n",
    "    feature_set.append(1 if '?' in tree else 0)            \n",
    "    features.append(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module re:\n",
      "\n",
      "NAME\n",
      "    re - Support for regular expressions (RE).\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.6/library/re\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    This module provides regular expression matching operations similar to\n",
      "    those found in Perl.  It supports both 8-bit and Unicode strings; both\n",
      "    the pattern and the strings being processed can contain null bytes and\n",
      "    characters outside the US ASCII range.\n",
      "    \n",
      "    Regular expressions can contain both special and ordinary characters.\n",
      "    Most ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\n",
      "    regular expressions; they simply match themselves.  You can\n",
      "    concatenate ordinary characters, so last matches the string 'last'.\n",
      "    \n",
      "    The special characters are:\n",
      "        \".\"      Matches any character except a newline.\n",
      "        \"^\"      Matches the start of the string.\n",
      "        \"$\"      Matches the end of the string or just before the newline at\n",
      "                 the end of the string.\n",
      "        \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n",
      "                 Greedy means that it will match as many repetitions as possible.\n",
      "        \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n",
      "        \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n",
      "        *?,+?,?? Non-greedy versions of the previous three special characters.\n",
      "        {m,n}    Matches from m to n repetitions of the preceding RE.\n",
      "        {m,n}?   Non-greedy version of the above.\n",
      "        \"\\\\\"     Either escapes special characters or signals a special sequence.\n",
      "        []       Indicates a set of characters.\n",
      "                 A \"^\" as the first character indicates a complementing set.\n",
      "        \"|\"      A|B, creates an RE that will match either A or B.\n",
      "        (...)    Matches the RE inside the parentheses.\n",
      "                 The contents can be retrieved or matched later in the string.\n",
      "        (?aiLmsux) Set the A, I, L, M, S, U, or X flag for the RE (see below).\n",
      "        (?:...)  Non-grouping version of regular parentheses.\n",
      "        (?P<name>...) The substring matched by the group is accessible by name.\n",
      "        (?P=name)     Matches the text matched earlier by the group named name.\n",
      "        (?#...)  A comment; ignored.\n",
      "        (?=...)  Matches if ... matches next, but doesn't consume the string.\n",
      "        (?!...)  Matches if ... doesn't match next.\n",
      "        (?<=...) Matches if preceded by ... (must be fixed length).\n",
      "        (?<!...) Matches if not preceded by ... (must be fixed length).\n",
      "        (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n",
      "                           the (optional) no pattern otherwise.\n",
      "    \n",
      "    The special sequences consist of \"\\\\\" and a character from the list\n",
      "    below.  If the ordinary character is not on the list, then the\n",
      "    resulting RE will match the second character.\n",
      "        \\number  Matches the contents of the group of the same number.\n",
      "        \\A       Matches only at the start of the string.\n",
      "        \\Z       Matches only at the end of the string.\n",
      "        \\b       Matches the empty string, but only at the start or end of a word.\n",
      "        \\B       Matches the empty string, but not at the start or end of a word.\n",
      "        \\d       Matches any decimal digit; equivalent to the set [0-9] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode digits.\n",
      "        \\D       Matches any non-digit character; equivalent to [^\\d].\n",
      "        \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v] in\n",
      "                 bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the whole\n",
      "                 range of Unicode whitespace characters.\n",
      "        \\S       Matches any non-whitespace character; equivalent to [^\\s].\n",
      "        \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_]\n",
      "                 in bytes patterns or string patterns with the ASCII flag.\n",
      "                 In string patterns without the ASCII flag, it will match the\n",
      "                 range of Unicode alphanumeric characters (letters plus digits\n",
      "                 plus underscore).\n",
      "                 With LOCALE, it will match the set [0-9_] plus characters defined\n",
      "                 as letters for the current locale.\n",
      "        \\W       Matches the complement of \\w.\n",
      "        \\\\       Matches a literal backslash.\n",
      "    \n",
      "    This module exports the following functions:\n",
      "        match     Match a regular expression pattern to the beginning of a string.\n",
      "        fullmatch Match a regular expression pattern to all of a string.\n",
      "        search    Search a string for the presence of a pattern.\n",
      "        sub       Substitute occurrences of a pattern found in a string.\n",
      "        subn      Same as sub, but also return the number of substitutions made.\n",
      "        split     Split a string by the occurrences of a pattern.\n",
      "        findall   Find all occurrences of a pattern in a string.\n",
      "        finditer  Return an iterator yielding a match object for each match.\n",
      "        compile   Compile a pattern into a RegexObject.\n",
      "        purge     Clear the regular expression cache.\n",
      "        escape    Backslash all non-alphanumerics in a string.\n",
      "    \n",
      "    Some of the functions in this module takes flags as optional parameters:\n",
      "        A  ASCII       For string patterns, make \\w, \\W, \\b, \\B, \\d, \\D\n",
      "                       match the corresponding ASCII character categories\n",
      "                       (rather than the whole Unicode categories, which is the\n",
      "                       default).\n",
      "                       For bytes patterns, this flag is the only available\n",
      "                       behaviour and needn't be specified.\n",
      "        I  IGNORECASE  Perform case-insensitive matching.\n",
      "        L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n",
      "        M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n",
      "                       as well as the string.\n",
      "                       \"$\" matches the end of lines (before a newline) as well\n",
      "                       as the end of the string.\n",
      "        S  DOTALL      \".\" matches any character at all, including the newline.\n",
      "        X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n",
      "        U  UNICODE     For compatibility only. Ignored for string patterns (it\n",
      "                       is the default), and forbidden for bytes patterns.\n",
      "    \n",
      "    This module also defines an exception 'error'.\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        sre_constants.error\n",
      "    \n",
      "    class error(builtins.Exception)\n",
      "     |  Exception raised for invalid regular expressions.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |  \n",
      "     |      msg: The unformatted error message\n",
      "     |      pattern: The regular expression pattern\n",
      "     |      pos: The index in the pattern where compilation failed (may be None)\n",
      "     |      lineno: The line corresponding to pos (may be None)\n",
      "     |      colno: The column corresponding to pos (may be None)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg, pattern=None, pos=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    compile(pattern, flags=0)\n",
      "        Compile a regular expression pattern, returning a pattern object.\n",
      "    \n",
      "    escape(pattern)\n",
      "        Escape all the characters in pattern except ASCII letters, numbers and '_'.\n",
      "    \n",
      "    findall(pattern, string, flags=0)\n",
      "        Return a list of all non-overlapping matches in the string.\n",
      "        \n",
      "        If one or more capturing groups are present in the pattern, return\n",
      "        a list of groups; this will be a list of tuples if the pattern\n",
      "        has more than one group.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    finditer(pattern, string, flags=0)\n",
      "        Return an iterator over all non-overlapping matches in the\n",
      "        string.  For each match, the iterator returns a match object.\n",
      "        \n",
      "        Empty matches are included in the result.\n",
      "    \n",
      "    fullmatch(pattern, string, flags=0)\n",
      "        Try to apply the pattern to all of the string, returning\n",
      "        a match object, or None if no match was found.\n",
      "    \n",
      "    match(pattern, string, flags=0)\n",
      "        Try to apply the pattern at the start of the string, returning\n",
      "        a match object, or None if no match was found.\n",
      "    \n",
      "    purge()\n",
      "        Clear the regular expression caches\n",
      "    \n",
      "    search(pattern, string, flags=0)\n",
      "        Scan through string looking for a match to the pattern, returning\n",
      "        a match object, or None if no match was found.\n",
      "    \n",
      "    split(pattern, string, maxsplit=0, flags=0)\n",
      "        Split the source string by the occurrences of the pattern,\n",
      "        returning a list containing the resulting substrings.  If\n",
      "        capturing parentheses are used in pattern, then the text of all\n",
      "        groups in the pattern are also returned as part of the resulting\n",
      "        list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
      "        and the remainder of the string is returned as the final element\n",
      "        of the list.\n",
      "    \n",
      "    sub(pattern, repl, string, count=0, flags=0)\n",
      "        Return the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in string by the\n",
      "        replacement repl.  repl can be either a string or a callable;\n",
      "        if a string, backslash escapes in it are processed.  If it is\n",
      "        a callable, it's passed the match object and must return\n",
      "        a replacement string to be used.\n",
      "    \n",
      "    subn(pattern, repl, string, count=0, flags=0)\n",
      "        Return a 2-tuple containing (new_string, number).\n",
      "        new_string is the string obtained by replacing the leftmost\n",
      "        non-overlapping occurrences of the pattern in the source\n",
      "        string by the replacement repl.  number is the number of\n",
      "        substitutions that were made. repl can be either a string or a\n",
      "        callable; if a string, backslash escapes in it are processed.\n",
      "        If it is a callable, it's passed the match object and must\n",
      "        return a replacement string to be used.\n",
      "    \n",
      "    template(pattern, flags=0)\n",
      "        Compile a template pattern, returning a pattern object\n",
      "\n",
      "DATA\n",
      "    A = <RegexFlag.ASCII: 256>\n",
      "    ASCII = <RegexFlag.ASCII: 256>\n",
      "    DOTALL = <RegexFlag.DOTALL: 16>\n",
      "    I = <RegexFlag.IGNORECASE: 2>\n",
      "    IGNORECASE = <RegexFlag.IGNORECASE: 2>\n",
      "    L = <RegexFlag.LOCALE: 4>\n",
      "    LOCALE = <RegexFlag.LOCALE: 4>\n",
      "    M = <RegexFlag.MULTILINE: 8>\n",
      "    MULTILINE = <RegexFlag.MULTILINE: 8>\n",
      "    S = <RegexFlag.DOTALL: 16>\n",
      "    U = <RegexFlag.UNICODE: 32>\n",
      "    UNICODE = <RegexFlag.UNICODE: 32>\n",
      "    VERBOSE = <RegexFlag.VERBOSE: 64>\n",
      "    X = <RegexFlag.VERBOSE: 64>\n",
      "    __all__ = ['match', 'fullmatch', 'search', 'sub', 'subn', 'split', 'fi...\n",
      "\n",
      "VERSION\n",
      "    2.2.1\n",
      "\n",
      "FILE\n",
      "    /Users/jalexander/Projects/python36/lib/python3.6/re.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.44      0.93      0.60        67\n",
      "        list       1.00      0.26      0.41        47\n",
      "     summary       0.67      0.12      0.20        51\n",
      "       yesno       0.92      0.98      0.95        61\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       226\n",
      "   macro avg       0.76      0.57      0.54       226\n",
      "weighted avg       0.74      0.62      0.56       226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "y = [label_to_class(str_labels, q_typ    e) for q_type in q_types]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.1)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "['factoid', 'list', 'summary', 'yesno']\n",
      "\n",
      "Label Analysis: factoid\n",
      "Count: 619\n",
      "Average question length: 62.19386106623586\n",
      "TFIDF NGRAM=(1, 1): {'the': 0.16654194013860546, 'is': 0.15025829977958918, 'of': 0.1338216003532015, 'which': 0.12809479531982085, 'what': 0.11914779216782946}\n",
      "TFIDF NGRAM=(2, 2): {'is the': 0.15156092242943828, 'what is': 0.13299239392559714, 'which is': 0.07966706660730298, 'of the': 0.06791421706890893, 'in the': 0.0445911791973339}\n",
      "TFIDF NGRAM=(3, 3): {'what is the': 0.16171374753014955, 'which is the': 0.10893336835082734, 'is used for': 0.030694668820678513, 'is associated with': 0.025095454518804256, 'which gene is': 0.022363120649383113}\n",
      "TFIDF NGRAM=(1, 3): {'the': 0.12029039191838244, 'is': 0.10700374616311188, 'which': 0.09631260876232565, 'of': 0.0949517547717784, 'what': 0.08765256725121949}\n",
      "\n",
      "Label Analysis: list\n",
      "Count: 485\n",
      "Average question length: 66.31958762886597\n",
      "TFIDF NGRAM=(1, 1): {'the': 0.1442447772351292, 'which': 0.12510783941073375, 'are': 0.1170656539094204, 'of': 0.11566326513681362, 'list': 0.09939267231241695}\n",
      "TFIDF NGRAM=(2, 2): {'are the': 0.10063489066123625, 'which are': 0.08559888156707067, 'of the': 0.060857007549245584, 'in the': 0.0489038829915285, 'what are': 0.03707236929723822}\n",
      "TFIDF NGRAM=(3, 3): {'which are the': 0.1380044840794311, 'what are the': 0.05270976590637905, 'are associated with': 0.02593898829301411, 'are the main': 0.018701625546017644, 'for treatment of': 0.017730998625898713}\n",
      "TFIDF NGRAM=(1, 3): {'the': 0.10445903484823224, 'which': 0.09247056369465845, 'list': 0.08426084822275108, 'of': 0.08425619018906474, 'are': 0.08230521780326643}\n",
      "\n",
      "Label Analysis: summary\n",
      "Count: 531\n",
      "Average question length: 55.847457627118644\n",
      "TFIDF NGRAM=(1, 1): {'what': 0.22292792849960186, 'is': 0.21205719019346247, 'the': 0.18988884264379063, 'of': 0.16068987182641797, 'in': 0.09467617598849572}\n",
      "TFIDF NGRAM=(2, 2): {'what is': 0.28596706622746115, 'is the': 0.18529084806176718, 'of the': 0.06370794994862712, 'role of': 0.05724201117042043, 'what are': 0.056212885378653585}\n",
      "TFIDF NGRAM=(3, 3): {'what is the': 0.23882100794733382, 'the role of': 0.05934388471104441, 'is the role': 0.058664979022872994, 'what are the': 0.04855826787134967, 'what is known': 0.03255442773091663}\n",
      "TFIDF NGRAM=(1, 3): {'what': 0.1623770354896537, 'is': 0.15407461824574184, 'what is': 0.1526363934729502, 'the': 0.1286187482276278, 'of': 0.10696178047663539}\n",
      "\n",
      "Label Analysis: yesno\n",
      "Count: 616\n",
      "Average question length: 62.15584415584416\n",
      "TFIDF NGRAM=(1, 1): {'is': 0.1460674284235725, 'the': 0.09936791787724557, 'in': 0.09438152025083746, 'of': 0.09358156712450469, 'for': 0.07587101193000143}\n",
      "TFIDF NGRAM=(2, 2): {'is there': 0.06293238149922532, 'involved in': 0.05688486731248608, 'associated with': 0.05371276134594294, 'is the': 0.05011518679214659, 'of the': 0.04341378336658281}\n",
      "TFIDF NGRAM=(3, 3): {'is there any': 0.031886780448617494, 'for treatment of': 0.028446008322894914, 'are there any': 0.01948051948051948, 'effective for treatment': 0.015916555018117153, 'is there an': 0.013338732647737583}\n",
      "TFIDF NGRAM=(1, 3): {'is': 0.12652918434464355, 'in': 0.08253080102355974, 'the': 0.08196067398060486, 'of': 0.07804740339133806, 'for': 0.06093396540459466}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFIDF DATA ANALYSIS\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "print(\"Labels\")\n",
    "print(str_labels)\n",
    "print()\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "for label in str_labels:\n",
    "    label_questions = [questions[idx] for idx, q_type in enumerate(q_types) if q_type == label]\n",
    "    count = len(label_questions)   \n",
    "    average_question_length = np.average([len(q) for q in label_questions])\n",
    "    tfidf_weights = []\n",
    "\n",
    "    for ngram in N_GRAMS:\n",
    "        weights = build_tfidf_weights(label_questions, MIN_DF, MAX_DF, ngram, TOP_N) \n",
    "        tfidf_weights.append(weights)\n",
    "    \n",
    "        if BUILD_WORD_CLOUD:\n",
    "            build_word_cloud(weights, f\"{label}_{ngram}_world_cloud.png\")\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Label Analysis: {label}\")        \n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Average question length: {average_question_length}\")\n",
    "    for idx, ngram in enumerate(N_GRAMS):\n",
    "        print(f\"TFIDF NGRAM={ngram}: {tfidf_weights[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL BUILDING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "question_vectors = build_vector_model(questions)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(question_vectors)\n",
    "pca_res = pca.transform(question_vectors)\n",
    "\n",
    "# Plotting result\n",
    "pyplot.cla(); pyplot.clf();\n",
    "fig = pyplot.figure(1, figsize=(10, 10))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "colors = {\n",
    "    'factoid': 'red', \n",
    "    'list': 'blue', \n",
    "    'summary': 'green', \n",
    "    'yesno': 'black'\n",
    "}\n",
    "indexes = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "series = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "\n",
    "def check_func(label):\n",
    "    l_series = series[label]\n",
    "    l_series.set_visible(not l_series.get_visible())\n",
    "\n",
    "[indexes[label].append(idx) for idx, label in enumerate(q_types)]\n",
    "\n",
    "for label in str_labels:\n",
    "    idx = indexes[label]\n",
    "    series[label] = ax.scatter(pca_res[idx, 0], pca_res[idx, 1], pca_res[idx, 2], color=colors[label], s=8, alpha=0.4)\n",
    "\n",
    "rax = pyplot.axes([0.05, 0.4, 0.15, 0.15])\n",
    "check = CheckButtons(rax, ('factoid', 'list', 'summary', 'yesno'), (True, True, True, True))        \n",
    "[rec.set_facecolor(colors[label]) for label, rec in zip(str_labels, check.rectangles)]\n",
    "check.on_clicked(check_func)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL TESTING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "X = build_vector_model(questions)\n",
    "str_labels = list(np.unique(q_types))\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "# PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train)\n",
    "X_train_tr = pca.transform(X_train)\n",
    "X_test_tr = pca.transform(X_test)\n",
    "\n",
    "# search for optimal SVM parameters using grid search with 3-fold cross validation\n",
    "Cs = np.logspace(0, 4, 4)\n",
    "gammas = np.logspace(0, 3, 4)\n",
    "param_grid = {'C': Cs, 'kernel': ['linear','rbf'], 'gamma': gammas}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=param_grid)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(clf.best_estimator_)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN()\n",
    "clf = KNeighborsClassifier(n_neighbors=4)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB()\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
