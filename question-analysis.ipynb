{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# DEPENDENCIES \n",
    "###\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import Tree\n",
    "# gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "# plotting\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.widgets import CheckButtons\n",
    "from wordcloud import WordCloud\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook  \n",
    "# stanford corenlp\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "### \n",
    "# GLOBALS \n",
    "###\n",
    "TRAINING_DATA_DIR=\"./datasets/BioASQ-trainingDataset6b.json\"\n",
    "# TFIDF CONFIG\n",
    "TOP_N=5\n",
    "MIN_DF=0.01\n",
    "MAX_DF=1.00\n",
    "N_GRAMS=[(1,1),(2,2),(3,3),(1,3)]\n",
    "BUILD_WORD_CLOUD=False\n",
    "# Stanford core-nlp config\n",
    "CORE_NLP_CLIENT = StanfordCoreNLP(r'/Users/jalexander/Projects/stanford-corenlp-full-2018-10-05')\n",
    "### \n",
    "# FUNCTIONS \n",
    "###\n",
    "def build_vector_model(questions):\n",
    "    questions = [tokenize(q) for q in questions]\n",
    "    model = Word2Vec(questions, min_count=0, sg=0)\n",
    "    question_vectors = []\n",
    "    for question in questions:\n",
    "        question_vector = []\n",
    "        for word in question:\n",
    "            question_vector = model[word] if (len(question_vector) == 0) else np.add(question_vector, model[word])\n",
    "        question_vectors.append(question_vector)\n",
    "    return question_vectors        \n",
    "\n",
    "def parse_questions_types(data):\n",
    "    return zip(*[[json['body'], json['type']] for json in data['questions']])\n",
    "\n",
    "def label_to_class(str_labels, label):\n",
    "    return str_labels.index(label)\n",
    "\n",
    "def build_tfidf_weights(sent_list, min_df, max_df, ngram, top_n=25):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df, \n",
    "        min_df=min_df, \n",
    "        tokenizer=tokenize, \n",
    "        ngram_range=ngram\n",
    "    )\n",
    "    tfidf = tfidf_vectorizer.fit_transform(sent_list)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    return top_mean_feats(tfidf, terms, top_n=top_n)\n",
    "\n",
    "def build_word_cloud(tfidf_weights, output_file):\n",
    "    # Initialize the word cloud\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        max_words=1000,\n",
    "        width = 1024,\n",
    "        height = 720,\n",
    "    )\n",
    "    wc.generate_from_frequencies(tfidf_weights)\n",
    "    wc.to_file(output_file)\n",
    "\n",
    "def json_to_df(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return pd.DataFrame(json.load(f))\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    feats = {}\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    for i in topn_ids:\n",
    "        feats[features[i]] = row[i]\n",
    "\n",
    "    return feats\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    filtered_tokens = []\n",
    "    for word, pos in nltk.pos_tag(word_tokenize(text)):\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        filtered_tokens.append(word.lower())        \n",
    "    return filtered_tokens\n",
    "\n",
    "def build_parse_tree(text):\n",
    "    text = text.rstrip().split('. ')[-1]\n",
    "    return CORE_NLP_CLIENT.parse(text).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-a919459c1f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_parse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfeature_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeature_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'List '\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-7453bcc001ab>\u001b[0m in \u001b[0;36mbuild_parse_tree\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_parse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'. '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCORE_NLP_CLIENT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/stanfordcorenlp/corenlp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos,parse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/stanfordcorenlp/corenlp.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, annotators, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Connection'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    522\u001b[0m         }\n\u001b[1;32m    523\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/python36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.6/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features = []\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "\n",
    "# first_words, counts = np.unique([q.split(' ')[0] for q in questions], return_counts=True)\n",
    "# [x for _,x in sorted(zip(Y,X))]\n",
    "\n",
    "for q in questions:\n",
    "    tree = build_parse_tree(q) \n",
    "    feature_set = []        \n",
    "    feature_set.append(1 if 'List ' in tree else 0)\n",
    "    feature_set.append(1 if 'Is ' in tree else 0)\n",
    "    feature_set.append(1 if 'WH' in tree.lower() else 0)\n",
    "    feature_set.append(1 if '?' in tree else 0)\n",
    "    features.append(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.39      0.87      0.54        60\n",
      "        list       0.75      0.33      0.46        54\n",
      "     summary       0.00      0.00      0.00        55\n",
      "       yesno       0.80      0.98      0.88        57\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       226\n",
      "   macro avg       0.49      0.55      0.47       226\n",
      "weighted avg       0.49      0.56      0.48       226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jalexander/Projects/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.1)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jalexander/Projects/python36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1.0, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.52      0.67      0.58        66\n",
      "        list       1.00      0.26      0.42        53\n",
      "     summary       0.63      0.88      0.73        50\n",
      "       yesno       0.96      0.96      0.96        57\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       226\n",
      "   macro avg       0.78      0.69      0.67       226\n",
      "weighted avg       0.77      0.69      0.67       226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search for optimal SVM parameters using grid search with 3-fold cross validation\n",
    "Cs = np.logspace(0, 4, 4)\n",
    "gammas = np.logspace(0, 3, 4)\n",
    "param_grid = {'C': Cs, 'kernel': ['linear','rbf'], 'gamma': gammas}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=param_grid)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(clf.best_estimator_)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factoid       0.00      0.00      0.00        60\n",
      "        list       0.33      0.94      0.49        54\n",
      "     summary       0.50      0.02      0.04        55\n",
      "       yesno       0.80      0.98      0.88        57\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       226\n",
      "   macro avg       0.41      0.49      0.35       226\n",
      "weighted avg       0.40      0.48      0.35       226\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jalexander/Projects/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# KNN()\n",
    "clf = KNeighborsClassifier(n_neighbors=4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "['factoid', 'list', 'summary', 'yesno']\n",
      "\n",
      "Label Analysis: factoid\n",
      "Count: 619\n",
      "Average question length: 62.19386106623586\n",
      "TFIDF NGRAM=(1, 1): {'the': 0.16654194013860546, 'is': 0.15025829977958918, 'of': 0.1338216003532015, 'which': 0.12809479531982085, 'what': 0.11914779216782946}\n",
      "TFIDF NGRAM=(2, 2): {'is the': 0.15156092242943828, 'what is': 0.13299239392559714, 'which is': 0.07966706660730298, 'of the': 0.06791421706890893, 'in the': 0.0445911791973339}\n",
      "TFIDF NGRAM=(3, 3): {'what is the': 0.16171374753014955, 'which is the': 0.10893336835082734, 'is used for': 0.030694668820678513, 'is associated with': 0.025095454518804256, 'which gene is': 0.022363120649383113}\n",
      "TFIDF NGRAM=(1, 3): {'the': 0.12029039191838244, 'is': 0.10700374616311188, 'which': 0.09631260876232565, 'of': 0.0949517547717784, 'what': 0.08765256725121949}\n",
      "\n",
      "Label Analysis: list\n",
      "Count: 485\n",
      "Average question length: 66.31958762886597\n",
      "TFIDF NGRAM=(1, 1): {'the': 0.1442447772351292, 'which': 0.12510783941073375, 'are': 0.1170656539094204, 'of': 0.11566326513681362, 'list': 0.09939267231241695}\n",
      "TFIDF NGRAM=(2, 2): {'are the': 0.10063489066123625, 'which are': 0.08559888156707067, 'of the': 0.060857007549245584, 'in the': 0.0489038829915285, 'what are': 0.03707236929723822}\n",
      "TFIDF NGRAM=(3, 3): {'which are the': 0.1380044840794311, 'what are the': 0.05270976590637905, 'are associated with': 0.02593898829301411, 'are the main': 0.018701625546017644, 'for treatment of': 0.017730998625898713}\n",
      "TFIDF NGRAM=(1, 3): {'the': 0.10445903484823224, 'which': 0.09247056369465845, 'list': 0.08426084822275108, 'of': 0.08425619018906474, 'are': 0.08230521780326643}\n",
      "\n",
      "Label Analysis: summary\n",
      "Count: 531\n",
      "Average question length: 55.847457627118644\n",
      "TFIDF NGRAM=(1, 1): {'what': 0.22292792849960186, 'is': 0.21205719019346247, 'the': 0.18988884264379063, 'of': 0.16068987182641797, 'in': 0.09467617598849572}\n",
      "TFIDF NGRAM=(2, 2): {'what is': 0.28596706622746115, 'is the': 0.18529084806176718, 'of the': 0.06370794994862712, 'role of': 0.05724201117042043, 'what are': 0.056212885378653585}\n",
      "TFIDF NGRAM=(3, 3): {'what is the': 0.23882100794733382, 'the role of': 0.05934388471104441, 'is the role': 0.058664979022872994, 'what are the': 0.04855826787134967, 'what is known': 0.03255442773091663}\n",
      "TFIDF NGRAM=(1, 3): {'what': 0.1623770354896537, 'is': 0.15407461824574184, 'what is': 0.1526363934729502, 'the': 0.1286187482276278, 'of': 0.10696178047663539}\n",
      "\n",
      "Label Analysis: yesno\n",
      "Count: 616\n",
      "Average question length: 62.15584415584416\n",
      "TFIDF NGRAM=(1, 1): {'is': 0.1460674284235725, 'the': 0.09936791787724557, 'in': 0.09438152025083746, 'of': 0.09358156712450469, 'for': 0.07587101193000143}\n",
      "TFIDF NGRAM=(2, 2): {'is there': 0.06293238149922532, 'involved in': 0.05688486731248608, 'associated with': 0.05371276134594294, 'is the': 0.05011518679214659, 'of the': 0.04341378336658281}\n",
      "TFIDF NGRAM=(3, 3): {'is there any': 0.031886780448617494, 'for treatment of': 0.028446008322894914, 'are there any': 0.01948051948051948, 'effective for treatment': 0.015916555018117153, 'is there an': 0.013338732647737583}\n",
      "TFIDF NGRAM=(1, 3): {'is': 0.12652918434464355, 'in': 0.08253080102355974, 'the': 0.08196067398060486, 'of': 0.07804740339133806, 'for': 0.06093396540459466}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFIDF DATA ANALYSIS\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "print(\"Labels\")\n",
    "print(str_labels)\n",
    "print()\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "for label in str_labels:\n",
    "    label_questions = [questions[idx] for idx, q_type in enumerate(q_types) if q_type == label]\n",
    "    count = len(label_questions)   \n",
    "    average_question_length = np.average([len(q) for q in label_questions])\n",
    "    tfidf_weights = []\n",
    "\n",
    "    for ngram in N_GRAMS:\n",
    "        weights = build_tfidf_weights(label_questions, MIN_DF, MAX_DF, ngram, TOP_N) \n",
    "        tfidf_weights.append(weights)\n",
    "    \n",
    "        if BUILD_WORD_CLOUD:\n",
    "            build_word_cloud(weights, f\"{label}_{ngram}_world_cloud.png\")\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Label Analysis: {label}\")        \n",
    "    print(f\"Count: {count}\")\n",
    "    print(f\"Average question length: {average_question_length}\")\n",
    "    for idx, ngram in enumerate(N_GRAMS):\n",
    "        print(f\"TFIDF NGRAM={ngram}: {tfidf_weights[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL BUILDING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "str_labels = list(np.unique(q_types))\n",
    "question_vectors = build_vector_model(questions)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(question_vectors)\n",
    "pca_res = pca.transform(question_vectors)\n",
    "\n",
    "# Plotting result\n",
    "pyplot.cla(); pyplot.clf();\n",
    "fig = pyplot.figure(1, figsize=(10, 10))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "colors = {\n",
    "    'factoid': 'red', \n",
    "    'list': 'blue', \n",
    "    'summary': 'green', \n",
    "    'yesno': 'black'\n",
    "}\n",
    "indexes = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "series = {\n",
    "    'factoid': [], \n",
    "    'list': [], \n",
    "    'summary': [], \n",
    "    'yesno': []\n",
    "}\n",
    "\n",
    "def check_func(label):\n",
    "    l_series = series[label]\n",
    "    l_series.set_visible(not l_series.get_visible())\n",
    "\n",
    "[indexes[label].append(idx) for idx, label in enumerate(q_types)]\n",
    "\n",
    "for label in str_labels:\n",
    "    idx = indexes[label]\n",
    "    series[label] = ax.scatter(pca_res[idx, 0], pca_res[idx, 1], pca_res[idx, 2], color=colors[label], s=8, alpha=0.4)\n",
    "\n",
    "rax = pyplot.axes([0.05, 0.4, 0.15, 0.15])\n",
    "check = CheckButtons(rax, ('factoid', 'list', 'summary', 'yesno'), (True, True, True, True))        \n",
    "[rec.set_facecolor(colors[label]) for label, rec in zip(str_labels, check.rectangles)]\n",
    "check.on_clicked(check_func)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "/Users/jalexander/Projects/python36/lib/python3.6/site-packages/ipykernel_launcher.py:59: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/jalexander/Projects/python36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# MODEL TESTING\n",
    "[questions, q_types] = parse_questions_types(json_to_df(TRAINING_DATA_DIR))\n",
    "X = build_vector_model(questions)\n",
    "str_labels = list(np.unique(q_types))\n",
    "y = [label_to_class(str_labels, q_type) for q_type in q_types]\n",
    "\n",
    "# PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_train)\n",
    "X_train_tr = pca.transform(X_train)\n",
    "X_test_tr = pca.transform(X_test)\n",
    "\n",
    "# search for optimal SVM parameters using grid search with 3-fold cross validation\n",
    "Cs = np.logspace(0, 4, 4)\n",
    "gammas = np.logspace(0, 3, 4)\n",
    "param_grid = {'C': Cs, 'kernel': ['linear','rbf'], 'gamma': gammas}\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=param_grid)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(clf.best_estimator_)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN()\n",
    "clf = KNeighborsClassifier(n_neighbors=4)\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB()\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train_tr, y_train)\n",
    "y_hat = clf.predict(X_test_tr)\n",
    "print(classification_report(y_test, y_hat, target_names=np.unique(q_types)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
